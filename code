import aiohttp
import aiofiles
import asyncio
from bs4 import BeautifulSoup
import time
import requests
import networkx as nx
from tqdm import tqdm

path = '/HW2.3/links.txt'
links_array = [link for link in open(path)]
for link in links_array: 
    print(link)

async def make_links_array(discription, url):
    try:
        parser = 'html.parser'
        links_array = []
        async with discription.get(url) as result:
            bs = BeautifulSoup(await result.text(), parser)
            for elem in bs.find_all('a', href=True):
              links_array.append(elem.get('href')) 
            return links_array
    except Exception as exception:
        print(f"Exept exception {url}: {exception}")
        return []

async def drop_to_file(urls):
    async with aiohttp.ClientSession() as discription:
        targets = []
        for url in urls:
            targets.append(make_links_array(discription, url)) 
        results = await asyncio.gather(*targets)
        async with aiofiles.open('target.txt', 'w') as file:
            for url, links in zip(urls, results):
                await file.write(f"link - {url}:\n")
                [await file.write(f"{elem}\n") for elem in links]
                await file.write("\n")

await drop_to_file(links_array)
